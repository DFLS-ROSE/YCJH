2024年3月31日 星期日

---

这周由于月考，完成的任务比较少，只是针对上周这份代码做了一个简单的程序流程图。

在这之中，我也了解到深度学习中的一些概念。

### 梯度

1. **定义**：梯度是一个向量，它描述了函数在某一点上变化最快的方向和速率。在深度学习的语境中，我们通常关心的是损失函数关于模型参数的梯度。
2. **作用**：在BERT或任何其他神经网络训练过程中，目标是最小化一个损失函数。为了做到这一点，我们采用梯度下降算法，其核心思想是：在每次迭代中，根据损失函数关于模型参数的梯度来更新参数，从而使损失函数值减小。
3. **计算**：在BERT中，损失函数通常是交叉熵损失或其他相关损失。为了计算这个损失函数关于模型参数的梯度，我们使用反向传播算法。这是一个从输出层到输入层的递归过程，用于计算损失函数关于每个参数的梯度。
4. **优化**：一旦有了梯度，就可以使用优化算法（如梯度下降）来更新模型参数。这个过程会反复进行，直到损失函数收敛到一个局部最小值或满足其他停止条件。

### Softmax函数

Softmax函数在深度学习和机器学习中非常常见，尤其是在处理多分类问题时。这个函数能够将一个实数向量映射为一个概率分布，即输出的每个值都在0和1之间，且所有值的和为1。

对于一个向量 z ，其中 z_i 是向量的第 i 个元素，Softmax函数将其转化为概率 p ，其公式如下：

$$
p_i=\dfrac{e^{z_i}}{\sum_je^{z_j}}
$$

这些概率随后用于计算交叉熵损失。

### 交叉熵损失函数

交叉熵损失（Cross-Entropy Loss）是深度学习中分类任务常用的一种损失函数。它度量了模型预测的概率分布与真实概率分布之间的差异。对于BERT这样的模型，交叉熵损失通常用于计算模型输出与真实标签之间的差异，并据此来更新模型参数。

在二分类模型当中，对于真实标签y（对应于输入文本的某些特定属性或类别）和正样本（预测结果正确）概率p，则交叉熵损失可定义为：

$$
L=−y\ln(p)−(1−y)\ln(1−p)
$$

多分类模型中也是类似：

$$
L_{i}=\frac{1}{N} \sum_{i=1}^N-\left[\,y_{i} \cdot \ln \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \ln \left(1-p_{i}\right)\,\right]
$$

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/3e343463-380d-4177-b6bf-a9a4cd724e17/34a54614-8f23-4807-b714-3d01d4fb8fbb/Untitled.png)

### 前向传播

前向传播是指输入数据通过神经网络各层逐层传播最后产生输出的过程。

在BERT的训练过程中，前向传播是每次迭代的第一步。它允许我们评估当前模型参数下的性能，并为接下来的反向传播和参数更新提供必要的信息。通过多次迭代和优化，模型能够逐渐改进其预测能力，最终达到或接近最佳性能。

在了解这些概念之后，我根据源代码制作了一张流程图。

[代码流程图.html](https://prod-files-secure.s3.us-west-2.amazonaws.com/3e343463-380d-4177-b6bf-a9a4cd724e17/809ee709-f702-4b25-807a-5bb60acd0fdb/%E4%BB%A3%E7%A0%81%E6%B5%81%E7%A8%8B%E5%9B%BE.html)

本周的研究就此结束。