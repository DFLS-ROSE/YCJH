2024.08.09报告（08.05-08.11）


1. 论文研读：***Infusing Knowledge from Wikipedia to Enhance Stance Detection***

    - 引言
        
        立场检测旨在自动识别文本作者对特定目标或话题的态度（支持、中立、反对）。然而，现有方法通常缺乏关于目标的背景知识，这在文本未明确提及目标时尤为关键。本文提出利用Wikipedia中的背景知识来弥补这一不足，从而提升立场检测的准确性。

    - 问题定义
        
        给定包含文档、目标和Wikipedia文本的输入，模型需要预测作者的态度（支持、反对、中立）。

    - 编码Wikipedia知识
        - **WS-BERT-Single**：用于正式文档，将文档、目标和Wikipedia知识合并为一个序列进行编码。
        - **WS-BERT-Dual**：用于非正式文档，如社交媒体文本，分别使用适合社交媒体文本的BERT模型和通用BERT模型对文档-目标对和Wikipedia知识进行编码。
  
    - 实验
        - 数据集
            - **P-Stance**：包含关于三位政治家的推特，用于目标特定和跨目标立场检测。
            - **COVID-19-Stance**：包含与COVID-19相关推文的数据集，用于目标特定立场检测。
            - **VAST**：包含《纽约时报》“Room for Debate”板块的评论，用于零/少样本立场检测。

        - 评价指标
        
            使用宏观平均F1分数作为评价指标。

        - 实验设置

            根据文档类型选择合适的WS-BERT变体，并使用Adam优化器进行训练。

        - 结果与分析
            - **目标特定立场检测**：在P-Stance和COVID-19-Stance数据集上，WS-BERT显著优于所有基线方法。
            - **跨目标立场检测**：在P-Stance数据集上进行跨目标立场检测时，WS-BERT同样表现出色，特别是在模型训练目标和测试目标差异较大时，Wikipedia知识带来的提升尤为明显。
            - **零/少样本立场检测**：在VAST数据集上，WS-BERT在零样本和少样本学习任务中均取得了显著优于基线方法的结果，证明了模型在处理未见目标时的泛化能力。

    - 结论
  
        本文提出了利用Wikipedia知识来增强立场检测的方法，并通过实验证明了该方法在多个基准数据集和子任务上的有效性。未来的工作将包括引入文档中实体的知识，以及探索如何减少Wikipedia中主观意见对模型的影响。


2. WS-BERT代码复现

    在运行`run_vast.py`之后，输出了一个字符串，用于训练模型：

    ```
    python3 -u src/train.py --data=vast --topic= --model=bert-base --wiki_model=bert-base --n_layers_freeze=10 --n_layers_freeze_wiki=0 --batch_size=32 --epochs=50 --patience=10 --lr=2e-05 --l2_reg=5e-05 --gpu=0 --inference=0  > results/vast-lr=2e-05-bs=32-n_layers_fz=10-wiki=bert-base-n_gpus=1.txt
    ```
    然而，在运行之后，程序没有输出任何内容，`vast-lr=2e-05-bs=32-n_layers_fz=10-wiki=bert-base-n_gpus=1.txt`中也没有任何数据。

    求助b站和AI工具均无果，只有下周再处理了。